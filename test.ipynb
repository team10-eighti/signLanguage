{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle = False\n",
    "\n",
    "def toggling():\n",
    "    global toggle\n",
    "    toggle = not toggle  # 상태 토글\n",
    "    return gr.update(visible=toggle), gr.update(visible=not toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test11 = f\"0번 : {completion.choices[0].message.context['intent'].split('\", \"')[0]}, 1번 : {completion.choices[0].message.context['intent'].split('\", \"')[1]}, 2번 : {completion.choices[0].message.context['intent'].split('\", \"')[2]}\"    # 0번 : [\"오토바이 수화, 1번 : 수화로 오토바이 표현, 2번 : 오토바이를 수화로 어떻게 표현하나요\"] 영어로 나올 때도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# CHATBOT ################# 다은님 / 예외 메시지, 클리어\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()  # 속성 접근 방식으로 수정\n",
    "        \n",
    "        # 검색 결과 없는 기본 메시지 -> 원하는 메세지로 예외처리\n",
    "        if \"요청된 정보는 검색된 데이터에서 찾을 수 없습니다. 다른 쿼리나 주제를 시도해 보세요.\" in assistant_reply:\n",
    "            assistant_reply = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        try:\n",
    "            citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "            video_url = citations[-2]\n",
    "            image_urls = citations[-3].split(' ')  # 이미지 URL이 여러 개일 수 있음\n",
    "        except (KeyError, IndexError):\n",
    "            # 출처 정보를 찾을 수 없을 경우 예외 처리\n",
    "            video_url = None\n",
    "            image_urls = []\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        # speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, chat_history, video_url, image_urls\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류 메시지 대체\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "        return chat_history, chat_history, None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "원형님 코드 음성 중지\n",
    "'''\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 한글 분리 함수 (2번 코드)\n",
    "def split_hangul(word):\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "\n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if ord('가') <= ord(char) <= ord('힣'):  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])  # 중성 추가\n",
    "            if final_consonants[final]:  # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    "\n",
    "    return separated\n",
    "\n",
    "# 한글 자음모음 & 커스텀비전 태그 연결 딕셔너리\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}  # 역딕셔너리 생성\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "def stop_speech():\n",
    "    \"\"\"음성을 멈추는 함수\"\"\"\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "# 채팅 함수 정의\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    try:\n",
    "        global is_speaking\n",
    "\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        assistant_reply = completion.choices[0].message.content.strip()\n",
    "        chat_history.append((user_input, assistant_reply))\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, chat_history\n",
    "\n",
    "    except Exception as e:\n",
    "        return [(\"Error\", str(e))], chat_history\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"# Azure OpenAI + Cognitive Search + Speech 기반 수화 챗봇\")\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")  # 음성을 멈추는 버튼 추가\n",
    "            decompose_button = gr.Button(\"Decompose Hangul\")\n",
    "\n",
    "        with gr.Column():\n",
    "            hangul_output = gr.Textbox(label=\"Hangul Decomposition Result\")\n",
    "\n",
    "    state = gr.State([])  # 대화 기록 저장\n",
    "\n",
    "    def decompose_hangul(user_input):\n",
    "        \"\"\"한글 자음/모음을 분리하는 함수\"\"\"\n",
    "        separated_word = split_hangul(user_input)\n",
    "        result_tags = [reverse_hangul_dict[char] for char in separated_word if char in reverse_hangul_dict]\n",
    "        return \", \".join(result_tags)\n",
    "\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, state])\n",
    "    clear_button.click(lambda: [], None, chatbot)\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추는 버튼과 함수 연결\n",
    "    decompose_button.click(decompose_hangul, inputs=user_input, outputs=hangul_output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-vision-customvision in c:\\python\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: msrest>=0.6.21 in c:\\python\\lib\\site-packages (from azure-cognitiveservices-vision-customvision) (0.7.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\python\\lib\\site-packages (from azure-cognitiveservices-vision-customvision) (1.1.28)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in c:\\python\\lib\\site-packages (from azure-cognitiveservices-vision-customvision) (1.5.0)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\python\\lib\\site-packages (from azure-mgmt-core<2.0.0,>=1.2.0->azure-cognitiveservices-vision-customvision) (1.32.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (2024.8.30)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\python\\lib\\site-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (0.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\python\\lib\\site-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (2.0.0)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\python\\lib\\site-packages (from msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\김재빈\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.31.0->azure-mgmt-core<2.0.0,>=1.2.0->azure-cognitiveservices-vision-customvision) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\python\\lib\\site-packages (from azure-core>=1.31.0->azure-mgmt-core<2.0.0,>=1.2.0->azure-cognitiveservices-vision-customvision) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (2.2.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-cognitiveservices-vision-customvision) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install azure-cognitiveservices-vision-customvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\python\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\python\\lib\\site-packages (from opencv-python) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\python\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: gradio in c:\\python\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: pillow in c:\\python\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\python\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: azure-cognitiveservices-vision-customvision in c:\\python\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: msrest in c:\\python\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\python\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\python\\lib\\site-packages (from gradio) (4.6.2.post1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\python\\lib\\site-packages (from gradio) (0.115.4)\n",
      "Requirement already satisfied: ffmpy in c:\\python\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.2 in c:\\python\\lib\\site-packages (from gradio) (1.4.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\python\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in c:\\python\\lib\\site-packages (from gradio) (0.26.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\python\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\python\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\python\\lib\\site-packages (from gradio) (3.10.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\김재빈\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\python\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\python\\lib\\site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in c:\\python\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Collecting python-multipart==0.0.12 (from gradio)\n",
      "  Obtaining dependency information for python-multipart==0.0.12 from https://files.pythonhosted.org/packages/f5/0b/c316262244abea7481f95f1e91d7575f3dfcf6455d56d1ffe9839c582eb1/python_multipart-0.0.12-py3-none-any.whl.metadata\n",
      "  Using cached python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\python\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\python\\lib\\site-packages (from gradio) (0.7.1)\n",
      "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in c:\\python\\lib\\site-packages (from gradio) (0.1.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\python\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\python\\lib\\site-packages (from gradio) (0.41.2)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\python\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\python\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\python\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\python\\lib\\site-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: fsspec in c:\\python\\lib\\site-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\python\\lib\\site-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\김재빈\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\python\\lib\\site-packages (from azure-cognitiveservices-vision-customvision) (1.1.28)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.2.0 in c:\\python\\lib\\site-packages (from azure-cognitiveservices-vision-customvision) (1.5.0)\n",
      "Requirement already satisfied: azure-core>=1.24.0 in c:\\python\\lib\\site-packages (from msrest) (1.32.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\python\\lib\\site-packages (from msrest) (0.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\python\\lib\\site-packages (from msrest) (2.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\김재빈\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.24.0->msrest) (1.16.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\python\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\python\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest) (3.2.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\python\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\python\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\python\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
      "Requirement already satisfied: colorama in c:\\python\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\김재빈\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Using cached python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: python-multipart\n",
      "  Attempting uninstall: python-multipart\n",
      "    Found existing installation: python-multipart 0.0.20\n",
      "    Uninstalling python-multipart-0.0.20:\n",
      "      Successfully uninstalled python-multipart-0.0.20\n",
      "Successfully installed python-multipart-0.0.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests gradio matplotlib pillow opencv-python numpy azure-cognitiveservices-vision-customvision msrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\python\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\gradio\\components\\base.py:201: UserWarning: 'scale' value should be an integer. Using 0.3 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\queueing.py\", line 669, in process_events\n",
      "    time_limit=cast(int, fn.time_limit) - first_iteration\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "영준님 코드\n",
    "이미지 폴더 필요\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import gradio as gr\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "import time\n",
    "\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    "\n",
    "# Initialize the prediction client\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "# 단어를 문자로 나누는 함수\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    separated = []  # 나눠진 문자가 들어갈 변수\n",
    "\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    "\n",
    "    return separated\n",
    "\n",
    "# 지문자 확인 버튼을 누르면 나눠진 문자를 이미지로 보여주는 함수\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]  # 영어 문자를 받는 변수 ex) Siot\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "        # output = [column1, column2, gallery, sign_confirmed_textbox]\n",
    "\n",
    "#################################\n",
    "# 카메라 인식 부분\n",
    "#################################\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def process_frame(frame, sign_word, sign_confirmed):\n",
    "\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        confirmed_list = list(sign_confirmed)\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            sign_confirmed = confirmed_list[i]\n",
    "\n",
    "            if sign_confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_list[i] = True\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "            i = i+1\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return frame, []\n",
    "\n",
    "# 채팅\n",
    "def request_gpt(user_text, temperature=0.7, top_p=0.95, max_tokens=800):\n",
    "\n",
    "    # endpoint\n",
    "    # method\n",
    "    # header\n",
    "    # payload\n",
    "\n",
    "    api_base=\"https://team10-eighti.openai.azure.com\" \n",
    "    deployment_id=\"gpt-4o\"  \n",
    "    endpoint = f\"{api_base}/openai/deployments/{deployment_id}/chat/completions?api-version=2024-08-01-preview\"\n",
    "    api_key = '1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw'\n",
    "    \n",
    "    search_endpoint=\"https://team10-eighti-search.search.windows.net\"\n",
    "    search_key = 'wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3'\n",
    "    search_index=\"sign-index\"\n",
    "    semantic_name = \"sign-semantic\"\n",
    "\n",
    "    method = requests.post\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application.json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 사용자가 수화 정보를 찾는데 도움을 주는 수화 AI 도우미입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_text\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stop\": None,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"semantic_configuration\": semantic_name,\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {},\n",
    "                    \"filter\": None,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    # print(response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content_text = response_json['choices'][0]['message']['content']\n",
    "\n",
    "        # citations = response_json['choices'][0]['message']['context']['citations'][0]['content'].split('\\n')\n",
    "        citations = response_json['choices'][0]['message']['context']['citations']\n",
    "        if len(citations) > 0:\n",
    "            citations_movie = get_citations(citations, \"mp4\")\n",
    "            citations_images = get_citations(citations, \"jpg\").split(' ')\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "            if (len(citations_images) > 0 ):\n",
    "                citations_images_first = get_image_url(citations_images[0])\n",
    "\n",
    "                print(citations_images_first)\n",
    "                if (len(citations_images) > 1 ):\n",
    "                    citations_images_second = citations_images[1]\n",
    "        else:\n",
    "            citations_movie = \"No Video\"\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "        return content_text, citations_movie, citations_images_first, citations_images_second\n",
    "    else:\n",
    "        return \"\", \"No Video\", None, None\n",
    "\n",
    "def get_citations(citations=\"\", extentions=\"mp4\"):\n",
    "    if len(citations) > 0 and len(citations[0]) > 0 and len(citations[0]['content']) > 0:\n",
    "        contents = citations[0]['content'].split('\\n')\n",
    "\n",
    "        for row in contents:\n",
    "            row_list = row.split('.')\n",
    "            if row_list[-1] == extentions:\n",
    "                print(\"extentions=\", extentions, \"|row=\",  row)\n",
    "                return row\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def get_image_url(url):\n",
    "\n",
    "    if not url.strip():\n",
    "        return None  # 빈값 처리\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # HTTP 에러 확인\n",
    "        img = Image.open(BytesIO(response.content))  # 이미지를 메모리에서 읽음\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching image: {str(e)}\"\n",
    "\n",
    "# 사용자 입력을 전송하는 함수\n",
    "def click_send(prompt, histories):\n",
    "    # history_list = get_history_messages(histories=histories)\n",
    "    # response_text, citation_html = request_gpt(prompt, history_list)\n",
    "    response_text, citations_movie, citations_images_first, citations_images_second = request_gpt(prompt)\n",
    "    histories.append((prompt, response_text))\n",
    "    return histories, \"\", citations_movie, citations_images_first, citations_images_second, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # 두 개의 상태\n",
    "    col1_state = gr.State(value=True)  # Column 1 초기 visible 상태\n",
    "    col2_state = gr.State(value=False)  # Column 2 초기 visible 상태\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label='채팅 기록')\n",
    "            with gr.Row():\n",
    "                input_textbox = gr.Textbox(label=\"\", scale=7)\n",
    "                send_button = gr.Button(\"전송\", scale=1)\n",
    "\n",
    "        # citation = gr.HTML(label='참조')\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"소맥\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "\n",
    "            with gr.Column(visible=True) as column1:\n",
    "                videio = gr.Video(label=\"Video Player\", autoplay=True)\n",
    "                with gr.Row():\n",
    "                    image_first = gr.Image(scale=0.3)\n",
    "                    image_second = gr.Image(scale=0.3)\n",
    "\n",
    "            with gr.Column(visible=False) as column2:\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    with gr.Column():  # Input webcam column\n",
    "                        webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                    with gr.Column():  # Output display column\n",
    "                        output = gr.Image(label=\"Detected Objects\")\n",
    "\n",
    "                # Set the process_frame function as the update function for webcam input\n",
    "                webcam_input.stream(process_frame, inputs=[webcam_input, sign_input_textbox, sign_confirmed_textbox], outputs=[output, gallery, sign_confirmed_textbox])\n",
    "                \n",
    "                demo.title = \"Azure Custom Vision Object Detection\"\n",
    "                demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "\n",
    "    input_textbox.submit(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    send_button.click(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# content_text, citations_list = request_gpt(\"된장찌개가 수화로 어떻게 돼?\", temperature=0.7, top_p=0.95, max_tokens=800)\n",
    "\n",
    "# print(content_text)\n",
    "# print(citations_list)\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\gradio\\utils.py:980: UserWarning: Expected 3 arguments for function <function process_frame at 0x000001DBECFBE660>, received 1.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\gradio\\utils.py:984: UserWarning: Expected at least 3 arguments for function <function process_frame at 0x000001DBECFBE660>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7895\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7895/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "원형님이 만드신 최종본\n",
    "'''\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    "\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    "\n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "# 채팅\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    # 수어가 있는 경우\n",
    "    try:\n",
    "        global is_speaking\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()  # 속성 접근 방식으로 수정\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        video_url = citations[-2]\n",
    "        image_urls = citations[-3].split(' ')  # 이미지 URL이 여러 개일 수 있음\n",
    "\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "        \n",
    "        return chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    # 지문자로 대체하는 경우\n",
    "    except Exception as e:\n",
    "        # 오류 메시지 대체\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "\n",
    "        return chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"Draw bounding boxes on the image based on predictions\"\"\"\n",
    "    img = image.copy()\n",
    "    for pred in predictions:\n",
    "        if pred.probability > 0.9 :\n",
    "            color = (255, 0, 0)\n",
    "            box = pred.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "        \n",
    "        # Draw rectangle\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "        \n",
    "        # Add label with confidence score\n",
    "            label = f\"{pred.tag_name}: {pred.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, pred.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "\n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name  = draw_boxes(frame, results.predictions)\n",
    "        \n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        confirmed_list = list(sign_confirmed)\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            sign_confirmed = confirmed_list[i]\n",
    "\n",
    "            if sign_confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\")\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\")\n",
    "                    confirmed_list[i] = True\n",
    "                else : \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "                i = i+1\n",
    "        \n",
    "        return annotated_frame, gallery_images, str(confirmed_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return annotated_frame, []\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "demo.title = \"Azure Custom Vision Object Detection\"\n",
    "demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "        with gr.Column(visible=False) as column1:   # 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "        with gr.Column(visible=False) as column2:   # 기본적으로 숨김\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"소맥\", scale=7, max_length=2)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            show_btn = gr.Button(\"힌트\")\n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "            gr.Textbox(\"이미지가 보여질 공간\")\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    webcam_input.stream(process_frame, inputs=webcam_input, outputs=webcam_output)\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\python\\Lib\\site-packages\\gradio\\components\\base.py:201: UserWarning: 'scale' value should be an integer. Using 0.3 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7899\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extentions= mp4 |row= http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20191028/632066/MOV000253800_700X466.mp4\n",
      "extentions= jpg |row= http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20151223/232818/IMG000227218_700X466.jpg http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20151223/232819/IMG000227219_700X466.jpg\n",
      "Error fetching image: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "영준님 최종 코드\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import gradio as gr\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "import time\n",
    "\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    "\n",
    "# Initialize the prediction client\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    "\n",
    "    return separated\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "        # logger.info(str(highest_prediction.tag_name) + \" \" + str(highest_prediction.probability))\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "#################################\n",
    "# 카메라 인식 부분\n",
    "#################################\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "\n",
    "\n",
    "        # for confirmed in sign_confirmed_str_list:\n",
    "        #     if confirmed == \"True\":\n",
    "        #         logger.info(str(sign_confirmed_str_list))\n",
    "\n",
    "        confirmed_result= []\n",
    "        for i in range(len(split_sign)):\n",
    "            confirmed_result.append(\"False\")\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            confirmed = (sign_confirmed_str_list[i] == 'True')\n",
    "            if confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result[i] = \"True\"\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"True\"\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"False\"\n",
    "            i = i+1\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "\n",
    "def request_gpt(user_text, temperature=0.7, top_p=0.95, max_tokens=800):\n",
    "\n",
    "    # endpoint\n",
    "    # method\n",
    "    # header\n",
    "    # payload\n",
    "\n",
    "    api_base=\"https://team10-eighti.openai.azure.com\" \n",
    "    deployment_id=\"gpt-4o\"  \n",
    "    endpoint = f\"{api_base}/openai/deployments/{deployment_id}/chat/completions?api-version=2024-08-01-preview\"\n",
    "    api_key = '1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw'\n",
    "    \n",
    "    search_endpoint=\"https://team10-eighti-search.search.windows.net\"\n",
    "    search_key = 'wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3'\n",
    "    search_index=\"sign-index\"\n",
    "    semantic_name = \"sign-semantic\"\n",
    "\n",
    "    method = requests.post\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application.json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 사용자가 수화 정보를 찾는데 도움을 주는 수화 AI 도우미입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_text\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stop\": None,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"semantic_configuration\": semantic_name,\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {},\n",
    "                    \"filter\": None,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    # print(response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content_text = response_json['choices'][0]['message']['content']\n",
    "\n",
    "        # citations = response_json['choices'][0]['message']['context']['citations'][0]['content'].split('\\n')\n",
    "        citations = response_json['choices'][0]['message']['context']['citations']\n",
    "        if len(citations) > 0:\n",
    "            citations_movie = get_citations(citations, \"mp4\")\n",
    "            citations_images = get_citations(citations, \"jpg\").split(' ')\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "            if (len(citations_images) > 0 ):\n",
    "                citations_images_first = get_image_url(citations_images[0])\n",
    "\n",
    "                print(citations_images_first)\n",
    "                if (len(citations_images) > 1 ):\n",
    "                    citations_images_second = citations_images[1]\n",
    "        else:\n",
    "            \n",
    "            citations_movie = \"No Video\"\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "        return content_text, citations_movie, citations_images_first, citations_images_second\n",
    "    else:\n",
    "        return \"\", \"No Video\", None, None\n",
    "\n",
    "def get_citations(citations=\"\", extentions=\"mp4\"):\n",
    "    if len(citations) > 0 and len(citations[0]) > 0 and len(citations[0]['content']) > 0:\n",
    "        contents = citations[0]['content'].split('\\n')\n",
    "\n",
    "        for row in contents:\n",
    "            row_list = row.split('.')\n",
    "            if row_list[-1] == extentions:\n",
    "                print(\"extentions=\", extentions, \"|row=\",  row)\n",
    "                return row\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def get_image_url(url):\n",
    "\n",
    "    if not url.strip():\n",
    "        return None  # 빈값 처리\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # HTTP 에러 확인\n",
    "        img = Image.open(BytesIO(response.content))  # 이미지를 메모리에서 읽음\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching image: {str(e)}\"\n",
    "\n",
    "def click_send(prompt, histories):\n",
    "    # history_list = get_history_messages(histories=histories)\n",
    "    # response_text, citation_html = request_gpt(prompt, history_list)\n",
    "    response_text, citations_movie, citations_images_first, citations_images_second = request_gpt(prompt)\n",
    "    histories.append((prompt, response_text))\n",
    "    return histories, \"\", citations_movie, citations_images_first, citations_images_second, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label='채팅 기록')\n",
    "            with gr.Row():\n",
    "                input_textbox = gr.Textbox(label=\"\", scale=7)\n",
    "                send_button = gr.Button(\"전송\", scale=1)\n",
    "\n",
    "        # citation = gr.HTML(label='참조')\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"소맥\", scale=7, max_length=2)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=False)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "\n",
    "            with gr.Column(visible=True) as column1:\n",
    "                videio = gr.Video(label=\"Video Player\", autoplay=True)\n",
    "                with gr.Row():\n",
    "                    image_first = gr.Image(scale=0.3)\n",
    "                    image_second = gr.Image(scale=0.3)\n",
    "\n",
    "            with gr.Column(visible=False) as column2:\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    gallery = gr.Gallery(columns=[6], rows=[1], show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130, container=False)\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    with gr.Column():  # Input webcam column\n",
    "                        webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                    with gr.Column():  # Output display column\n",
    "                        output = gr.Image(label=\"Detected Objects\")\n",
    "\n",
    "                # Set the process_frame function as the update function for webcam input\n",
    "                webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[output, gallery, sign_confirmed_textbox])\n",
    "                \n",
    "                demo.title = \"Azure Custom Vision Object Detection\"\n",
    "                demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "\n",
    "    input_textbox.submit(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    send_button.click(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# content_text, citations_list = request_gpt(\"된장찌개가 수화로 어떻게 돼?\", temperature=0.7, top_p=0.95, max_tokens=800)\n",
    "\n",
    "# print(content_text)\n",
    "# print(citations_list)\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "원형님이 만드신 최종본에 영준님 최종본 추가하기\n",
    "'''\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    "\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    "\n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "# 채팅\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    # 수어가 있는 경우\n",
    "    try:\n",
    "        global is_speaking\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()  # 속성 접근 방식으로 수정\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        video_url = citations[-2]\n",
    "        image_urls = citations[-3].split(' ')  # 이미지 URL이 여러 개일 수 있음\n",
    "\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "        \n",
    "        return chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    # 지문자로 대체하는 경우\n",
    "    except Exception as e:\n",
    "        # 오류 메시지 대체\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "\n",
    "        return chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"Draw bounding boxes on the image based on predictions\"\"\"\n",
    "    img = image.copy()\n",
    "    for pred in predictions:\n",
    "        if pred.probability > 0.9 :\n",
    "            color = (255, 0, 0)\n",
    "            box = pred.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "        \n",
    "        # Draw rectangle\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "        \n",
    "        # Add label with confidence score\n",
    "            label = f\"{pred.tag_name}: {pred.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, pred.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "\n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name  = draw_boxes(frame, results.predictions)\n",
    "        \n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        confirmed_list = list(sign_confirmed)\n",
    "        \n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            sign_confirmed = confirmed_list[i]\n",
    "\n",
    "            if sign_confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\")\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\")\n",
    "                    confirmed_list[i] = True\n",
    "                else : \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "                i = i+1\n",
    "        \n",
    "        return annotated_frame, gallery_images, str(confirmed_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return annotated_frame, []\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "demo.title = \"Azure Custom Vision Object Detection\"\n",
    "demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "\n",
    "        with gr.Column():   # 채팅 섹션\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "\n",
    "        with gr.Column(visible=False) as column1:   # 수어 있는 경우 섹션, 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "\n",
    "        with gr.Column(visible=False) as column2:   # 지문자로 할 경우 섹션, 기본적으로 숨김\n",
    "\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"소맥\", scale=7, max_length=2)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            show_btn = gr.Button(\"힌트\")\n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "            gr.Textbox(\"이미지가 보여질 공간\")\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    webcam_input.stream(process_frame, inputs=webcam_input, outputs=webcam_output)\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "원형님 마지막 코드 공유\n",
    "'''\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "# 채팅\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    # 수어가 있는 경우\n",
    "    try:\n",
    "        global is_speaking\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()  # 속성 접근 방식으로 수정\n",
    "\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        video_url = citations[-2]\n",
    "        image_urls = citations[-3].split(' ')  # 이미지 URL이 여러 개일 수 있음\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    # 지문자로 대체하는 경우\n",
    "    except Exception as e:\n",
    "        # 오류 메시지 대체\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "\n",
    "        return chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"Draw bounding boxes on the image based on predictions\"\"\"\n",
    "    img = image.copy()\n",
    "    for pred in predictions:\n",
    "        if pred.probability > 0.9 :\n",
    "            color = (255, 0, 0)\n",
    "            box = pred.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "        \n",
    "        # Draw rectangle\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "        \n",
    "        # Add label with confidence score\n",
    "            label = f\"{pred.tag_name}: {pred.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, pred.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        # logger.info(sign_confirmed)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "        # logger.info(sign_confirmed)\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "            # logger.info(sign_confirmed)\n",
    "\n",
    "        logger.info(sign_confirmed_str_list)\n",
    "\n",
    "        # for confirmed in sign_confirmed_str_list:\n",
    "        #     if confirmed == \"True\":\n",
    "        #         logger.info(str(sign_confirmed_str_list))\n",
    "\n",
    "        confirmed_result= []\n",
    "        for i in range(len(split_sign)):\n",
    "            confirmed_result.append(\"False\")\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            confirmed = (sign_confirmed_str_list[i] == 'True')\n",
    "            # logger.info(confirmed)            \n",
    "            if confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result[i] = \"True\"\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"True\"\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"False\"\n",
    "            i = i+1\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "        with gr.Column(visible=False) as column1:   # 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "        with gr.Column(visible=False) as column2:   # 기본적으로 숨김\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            show_btn = gr.Button(\"힌트\")\n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "            demo.title = \"Azure Custom Vision Object Detection\"\n",
    "            demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "            gr.Textbox(\"이미지가 보여질 공간\")\n",
    "\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
