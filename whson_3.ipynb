{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\base.py:201: UserWarning: 'scale' value should be an integer. Using 0.3 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import gradio as gr\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "from loguru import logger\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "import time\n",
    "\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "# Initialize the prediction client\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "        logger.info(str(highest_prediction.tag_name) + \" \" + str(highest_prediction.probability))\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "#################################\n",
    "# 카메라 인식 부분\n",
    "#################################\n",
    "def process_frame(frame, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        confirmed_list = list(sign_confirmed)\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            sign_confirmed = confirmed_list[i]\n",
    "\n",
    "            if sign_confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_list[i] = True\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "            i = i+1\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, []\n",
    "\n",
    "\n",
    "def request_gpt(user_text, temperature=0.7, top_p=0.95, max_tokens=800):\n",
    "\n",
    "    # endpoint\n",
    "    # method\n",
    "    # header\n",
    "    # payload\n",
    "\n",
    "    api_base=\"https://team10-eighti.openai.azure.com\" \n",
    "    deployment_id=\"gpt-4o\"  \n",
    "    endpoint = f\"{api_base}/openai/deployments/{deployment_id}/chat/completions?api-version=2024-08-01-preview\"\n",
    "    api_key = '1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw'\n",
    "    \n",
    "    search_endpoint=\"https://team10-eighti-search.search.windows.net\"\n",
    "    search_key = 'wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3'\n",
    "    search_index=\"sign-index\"\n",
    "    semantic_name = \"sign-semantic\"\n",
    "\n",
    "    method = requests.post\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application.json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 사용자가 수화 정보를 찾는데 도움을 주는 수화 AI 도우미입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_text\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stop\": None,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"semantic_configuration\": semantic_name,\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {},\n",
    "                    \"filter\": None,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    # print(response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content_text = response_json['choices'][0]['message']['content']\n",
    "        logger.info(f\"채팅 답변 = {content_text}\")\n",
    "\n",
    "        # citations = response_json['choices'][0]['message']['context']['citations'][0]['content'].split('\\n')\n",
    "        citations = response_json['choices'][0]['message']['context']['citations']\n",
    "        if len(citations) > 0:\n",
    "            logger.info(f\"citations = {citations}\")\n",
    "            citations_movie = get_citations(citations, \"mp4\")\n",
    "            citations_images = get_citations(citations, \"jpg\").split(' ')\n",
    "            logger.info(f\"citations_movie = {citations_movie}\")\n",
    "            logger.info(f\"citations_images = {citations_images}\")\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "            if (len(citations_images) > 0 ):\n",
    "                citations_images_first = get_image_url(citations_images[0])\n",
    "\n",
    "                print(citations_images_first)\n",
    "                if (len(citations_images) > 1 ):\n",
    "                    citations_images_second = citations_images[1]\n",
    "        else:\n",
    "            logger.info(f\"citations(empty) = {citations}\")\n",
    "            \n",
    "            citations_movie = \"No Video\"\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "        return content_text, citations_movie, citations_images_first, citations_images_second\n",
    "    else:\n",
    "        return \"\", \"No Video\", None, None\n",
    "\n",
    "def get_citations(citations=\"\", extentions=\"mp4\"):\n",
    "    logger.info(f\"****get_citations = {citations}\")\n",
    "    if len(citations) > 0 and len(citations[0]) > 0 and len(citations[0]['content']) > 0:\n",
    "        contents = citations[0]['content'].split('\\n')\n",
    "\n",
    "        for row in contents:\n",
    "            row_list = row.split('.')\n",
    "            if row_list[-1] == extentions:\n",
    "                print(\"extentions=\", extentions, \"|row=\",  row)\n",
    "                return row\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def get_image_url(url):\n",
    "\n",
    "    if not url.strip():\n",
    "        return None  # 빈값 처리\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # HTTP 에러 확인\n",
    "        img = Image.open(BytesIO(response.content))  # 이미지를 메모리에서 읽음\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching image: {str(e)}\"\n",
    "\n",
    "def click_send(prompt, histories):\n",
    "    # history_list = get_history_messages(histories=histories)\n",
    "    # response_text, citation_html = request_gpt(prompt, history_list)\n",
    "    response_text, citations_movie, citations_images_first, citations_images_second = request_gpt(prompt)\n",
    "    histories.append((prompt, response_text))\n",
    "    return histories, \"\", citations_movie, citations_images_first, citations_images_second, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # 두 개의 상태\n",
    "    col1_state = gr.State(value=True)  # Column 1 초기 visible 상태\n",
    "    col2_state = gr.State(value=False)  # Column 2 초기 visible 상태\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label='채팅 기록')\n",
    "            with gr.Row():\n",
    "                input_textbox = gr.Textbox(label=\"\", scale=7)\n",
    "                send_button = gr.Button(\"전송\", scale=1)\n",
    "\n",
    "        # citation = gr.HTML(label='참조')\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"소맥\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "\n",
    "            with gr.Column(visible=True) as column1:\n",
    "                videio = gr.Video(label=\"Video Player\", autoplay=True)\n",
    "                with gr.Row():\n",
    "                    image_first = gr.Image(scale=0.3)\n",
    "                    image_second = gr.Image(scale=0.3)\n",
    "\n",
    "            with gr.Column(visible=False) as column2:\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    with gr.Column():  # Input webcam column\n",
    "                        webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                    with gr.Column():  # Output display column\n",
    "                        output = gr.Image(label=\"Detected Objects\")\n",
    "\n",
    "                # Set the process_frame function as the update function for webcam input\n",
    "                webcam_input.stream(process_frame, inputs=[webcam_input, sign_input_textbox, sign_confirmed_textbox], outputs=[output, gallery, sign_confirmed_textbox])\n",
    "                \n",
    "                demo.title = \"Azure Custom Vision Object Detection\"\n",
    "                demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "\n",
    "    input_textbox.submit(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    send_button.click(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# content_text, citations_list = request_gpt(\"된장찌개가 수화로 어떻게 돼?\", temperature=0.7, top_p=0.95, max_tokens=800)\n",
    "\n",
    "# print(content_text)\n",
    "# print(citations_list)\n",
    "\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\base.py:201: UserWarning: 'scale' value should be an integer. Using 0.3 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-20 17:33:03.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclick_sign_send\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1m소맥 ['ㅅ', 'ㅗ', 'ㅁ', 'ㅐ', 'ㄱ']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:26.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['False', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:26.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['False', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:27.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:28.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:28.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:29.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:30.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:35.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-20 17:33:35.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m148\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gradio as gr\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "from loguru import logger\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "import time\n",
    "\n",
    "\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "# Initialize the prediction client\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "        # logger.info(str(highest_prediction.tag_name) + \" \" + str(highest_prediction.probability))\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "#################################\n",
    "# 카메라 인식 부분\n",
    "#################################\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        # logger.info(sign_confirmed)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "        # logger.info(sign_confirmed)\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "            # logger.info(sign_confirmed)\n",
    "\n",
    "        logger.info(sign_confirmed_str_list)\n",
    "\n",
    "        # for confirmed in sign_confirmed_str_list:\n",
    "        #     if confirmed == \"True\":\n",
    "        #         logger.info(str(sign_confirmed_str_list))\n",
    "\n",
    "        confirmed_result= []\n",
    "        for i in range(len(split_sign)):\n",
    "            confirmed_result.append(\"False\")\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            confirmed = (sign_confirmed_str_list[i] == 'True')\n",
    "            # logger.info(confirmed)            \n",
    "            if confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result[i] = \"True\"\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"True\"\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"False\"\n",
    "            i = i+1\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "\n",
    "def request_gpt(user_text, temperature=0.7, top_p=0.95, max_tokens=800):\n",
    "\n",
    "    # endpoint\n",
    "    # method\n",
    "    # header\n",
    "    # payload\n",
    "\n",
    "    api_base=\"https://team10-eighti.openai.azure.com\" \n",
    "    deployment_id=\"gpt-4o\"  \n",
    "    endpoint = f\"{api_base}/openai/deployments/{deployment_id}/chat/completions?api-version=2024-08-01-preview\"\n",
    "    api_key = '1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw'\n",
    "    \n",
    "    search_endpoint=\"https://team10-eighti-search.search.windows.net\"\n",
    "    search_key = 'wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3'\n",
    "    search_index=\"sign-index\"\n",
    "    semantic_name = \"sign-semantic\"\n",
    "\n",
    "    method = requests.post\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application.json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 사용자가 수화 정보를 찾는데 도움을 주는 수화 AI 도우미입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_text\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stop\": None,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"semantic_configuration\": semantic_name,\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {},\n",
    "                    \"filter\": None,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    # print(response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content_text = response_json['choices'][0]['message']['content']\n",
    "        logger.info(f\"채팅 답변 = {content_text}\")\n",
    "\n",
    "        # citations = response_json['choices'][0]['message']['context']['citations'][0]['content'].split('\\n')\n",
    "        citations = response_json['choices'][0]['message']['context']['citations']\n",
    "        if len(citations) > 0:\n",
    "            logger.info(f\"citations = {citations}\")\n",
    "            citations_movie = get_citations(citations, \"mp4\")\n",
    "            citations_images = get_citations(citations, \"jpg\").split(' ')\n",
    "            logger.info(f\"citations_movie = {citations_movie}\")\n",
    "            logger.info(f\"citations_images = {citations_images}\")\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "            if (len(citations_images) > 0 ):\n",
    "                citations_images_first = get_image_url(citations_images[0])\n",
    "\n",
    "                print(citations_images_first)\n",
    "                if (len(citations_images) > 1 ):\n",
    "                    citations_images_second = citations_images[1]\n",
    "        else:\n",
    "            logger.info(f\"citations(empty) = {citations}\")\n",
    "            \n",
    "            citations_movie = \"No Video\"\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "        return content_text, citations_movie, citations_images_first, citations_images_second\n",
    "    else:\n",
    "        return \"\", \"No Video\", None, None\n",
    "\n",
    "def get_citations(citations=\"\", extentions=\"mp4\"):\n",
    "    logger.info(f\"****get_citations = {citations}\")\n",
    "    if len(citations) > 0 and len(citations[0]) > 0 and len(citations[0]['content']) > 0:\n",
    "        contents = citations[0]['content'].split('\\n')\n",
    "\n",
    "        for row in contents:\n",
    "            row_list = row.split('.')\n",
    "            if row_list[-1] == extentions:\n",
    "                print(\"extentions=\", extentions, \"|row=\",  row)\n",
    "                return row\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def get_image_url(url):\n",
    "\n",
    "    if not url.strip():\n",
    "        return None  # 빈값 처리\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # HTTP 에러 확인\n",
    "        img = Image.open(BytesIO(response.content))  # 이미지를 메모리에서 읽음\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching image: {str(e)}\"\n",
    "\n",
    "def click_send(prompt, histories):\n",
    "    # history_list = get_history_messages(histories=histories)\n",
    "    # response_text, citation_html = request_gpt(prompt, history_list)\n",
    "    response_text, citations_movie, citations_images_first, citations_images_second = request_gpt(prompt)\n",
    "    histories.append((prompt, response_text))\n",
    "    return histories, \"\", citations_movie, citations_images_first, citations_images_second, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label='채팅 기록')\n",
    "            with gr.Row():\n",
    "                input_textbox = gr.Textbox(label=\"\", scale=7)\n",
    "                send_button = gr.Button(\"전송\", scale=1)\n",
    "\n",
    "        # citation = gr.HTML(label='참조')\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"소맥\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=False)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "\n",
    "            with gr.Column(visible=True) as column1:\n",
    "                videio = gr.Video(label=\"Video Player\", autoplay=True)\n",
    "                with gr.Row():\n",
    "                    image_first = gr.Image(scale=0.3)\n",
    "                    image_second = gr.Image(scale=0.3)\n",
    "\n",
    "            with gr.Column(visible=False) as column2:\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    with gr.Column():  # Input webcam column\n",
    "                        webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                    with gr.Column():  # Output display column\n",
    "                        output = gr.Image(label=\"Detected Objects\")\n",
    "\n",
    "                # Set the process_frame function as the update function for webcam input\n",
    "                webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[output, gallery, sign_confirmed_textbox])\n",
    "                \n",
    "                demo.title = \"Azure Custom Vision Object Detection\"\n",
    "                demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "\n",
    "    input_textbox.submit(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    send_button.click(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second, column1, column2])\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# content_text, citations_list = request_gpt(\"된장찌개가 수화로 어떻게 돼?\", temperature=0.7, top_p=0.95, max_tokens=800)\n",
    "\n",
    "# print(content_text)\n",
    "# print(citations_list)\n",
    "\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
