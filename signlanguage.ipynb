{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "된장찌개를 수화로 표현할 때는 오른손의 손가락을 왼손의 5지와 1·2·3·4지 사이에 넣고 오른손 등이 스치게 한 다음, 손바닥이 위로 향하게 하여 약간 굽힌 왼손 밑에서 같은 모양의 오른손을 두 번 오르내립니다 [doc1].\n",
      "['음식', '손끝이 아래로 향하게 구부린 오른손의 손가락을 왼손의 5지와 1·2·3·4지 사이에 넣고 오른손 등이 스치게 뺀 다음, 손바닥이 위로 향하게 펴서 약간 구부린 왼손 밑에서 같은 모양의 오른손을 두 번 오르내린다.', 'http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20201224/791296/IMG000357950_700X466.jpg http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20151126/217119/IMG000208379_700X466.jpg', 'http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20201210/784487/MOV000356911_700X466.mp4', '된장찌개']\n"
     ]
    }
   ],
   "source": [
    "def request_gpt(user_text, temperature=0.7, top_p=0.95, max_tokens=800):\n",
    "\n",
    "    # endpoint\n",
    "    # method\n",
    "    # header\n",
    "    # payload\n",
    "\n",
    "    api_base=\"https://team10-eighti.openai.azure.com\" \n",
    "    deployment_id=\"gpt-4o\"  \n",
    "    endpoint = f\"{api_base}/openai/deployments/{deployment_id}/chat/completions?api-version=2024-08-01-preview\"\n",
    "    api_key = '1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw'\n",
    "    \n",
    "    search_endpoint=\"https://team10-eighti-search.search.windows.net\"\n",
    "    search_key = 'wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3'\n",
    "    search_index=\"sign-index\"\n",
    "    semantic_name = \"sign-semantic\"\n",
    "\n",
    "    method = requests.post\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application.json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 사용자가 수화 정보를 찾는데 도움을 주는 수화 AI 도우미입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_text\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stop\": None,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"semantic_configuration\": semantic_name,\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {},\n",
    "                    \"filter\": None,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    # print(response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content_text = response_json['choices'][0]['message']['content']\n",
    "        citations = response_json['choices'][0]['message']['context']['citations'][0]['content'].split('\\n')\n",
    "        return content_text, citations\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "content_text, citations_list = request_gpt(\"된장찌개가 수화로 어떻게 돼?\", temperature=0.7, top_p=0.95, max_tokens=800)\n",
    "\n",
    "print(content_text)\n",
    "print(citations_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import io\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "# Initialize the prediction client\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"Draw bounding boxes on the image based on predictions\"\"\"\n",
    "    img = image.copy()\n",
    "    for pred in predictions:\n",
    "        if pred.probability > 0.9 :\n",
    "            color = (255, 0, 0)\n",
    "            box = pred.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "        \n",
    "        # Draw rectangle\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "        \n",
    "        # Add label with confidence score\n",
    "            label = f\"{pred.tag_name}: {pred.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def process_frame(frame):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame = draw_boxes(frame, results.predictions)\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return frame\n",
    "\n",
    "\n",
    "# Checklist section\n",
    "with gr.Blocks() as demo:  \n",
    "            \n",
    "    interface = gr.Interface(\n",
    "        fn=process_frame,\n",
    "        inputs=gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True),\n",
    "        outputs=gr.Image(label=\"Detected Objects\"),\n",
    "        live=True,\n",
    "        title=\"Azure Custom Vision Object Detection\",\n",
    "        description=\"Real-time object detection using Azure Custom Vision\",\n",
    "        examples=[],\n",
    "        cache_examples=False\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 채팅 함수 정의\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content  # 속성 접근 방식으로 수정\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, chat_history\n",
    "\n",
    "    except Exception as e:\n",
    "        return [(\"Error\", str(e))], chat_history\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Azure OpenAI + Cognitive Search + Speech 기반 수화 챗봇\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "    clear_button = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, state])\n",
    "    clear_button.click(lambda: [], None, chatbot)\n",
    "\n",
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Gallery' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m references_button \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mButton(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m참고 자료 보기\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# 참고 자료 (이미지 및 동영상)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m images_output \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGallery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m참고 이미지\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstyle\u001b[49m(grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     92\u001b[0m videos_output \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mVideo(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m참고 영상\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 대화 기록 저장\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Gallery' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 채팅 함수 정의\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content\n",
    "        chat_history.append((user_input, assistant_reply))  # 대화 기록 저장\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, assistant_reply\n",
    "\n",
    "    except Exception as e:\n",
    "        return [(\"Error\", str(e))], chat_history\n",
    "\n",
    "# 참고 자료 검색 및 로드 함수\n",
    "def fetch_references(user_input):\n",
    "    try:\n",
    "        # 검색된 사진과 동영상 URL (샘플 데이터베이스나 API 연동 필요)\n",
    "        references = {\n",
    "            \"images\": [\n",
    "                \"https://example.com/image1.jpg\",\n",
    "                \"https://example.com/image2.jpg\"\n",
    "            ],\n",
    "            \"videos\": [\n",
    "                \"https://example.com/video1.mp4\",\n",
    "                \"https://example.com/video2.mp4\"\n",
    "            ]\n",
    "        }\n",
    "        return references[\"images\"], references[\"videos\"]\n",
    "    except Exception as e:\n",
    "        return [], []\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Azure OpenAI + Cognitive Search + Speech 기반 수화 학습 챗봇\")\n",
    "    \n",
    "    # 채팅창 구성\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "    clear_button = gr.Button(\"Clear Chat\")\n",
    "    references_button = gr.Button(\"참고 자료 보기\")\n",
    "\n",
    "    # 참고 자료 (이미지 및 동영상)\n",
    "    images_output = gr.Gallery(label=\"참고 이미지\").style(grid=2)\n",
    "    videos_output = gr.Video(label=\"참고 영상\")\n",
    "    \n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, state])\n",
    "    clear_button.click(lambda: [], None, chatbot)\n",
    "    references_button.click(fetch_references, [user_input], [images_output, videos_output])\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = \"이거 사용.CSV\"\n",
    "\n",
    "# CSV 파일 읽기\n",
    "data = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 단어 리스트 추출\n",
    "word_list = data[\"단어명\"].tolist()\n",
    "\n",
    "# 단어에 따른 동영상과 이미지 불러오기 함수\n",
    "def fetch_media(selected_word):\n",
    "    # 선택된 단어에 해당하는 행 필터링\n",
    "    row = data[data[\"단어명\"] == selected_word]\n",
    "    \n",
    "    if not row.empty:\n",
    "        video_url = row[\"동영상\"].values[0]\n",
    "        image_url = row[\"이미지\"].values[0]\n",
    "        description = row[\"설명\"].values[0]\n",
    "        return video_url, image_url, description\n",
    "    else:\n",
    "        return None, None, \"해당 단어를 찾을 수 없습니다.\"\n",
    "\n",
    "# 채팅 함수 정의\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content\n",
    "        chat_history.append((user_input, assistant_reply))  # 대화 기록 저장\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, chat_history\n",
    "\n",
    "    except Exception as e:\n",
    "        return [(\"Error\", str(e))], chat_history\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Azure OpenAI + 수어 학습 프로그램\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # 왼쪽: 채팅 및 음성 기능\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            # 대화 기록 저장\n",
    "            state = gr.State([])\n",
    "\n",
    "            # 이벤트 연결\n",
    "            user_input.submit(chat_with_openai, [user_input, state], [chatbot, state])\n",
    "            clear_button.click(lambda: [], None, chatbot)\n",
    "\n",
    "        # 오른쪽: 단어 선택, 동영상, 이미지 출력\n",
    "        with gr.Column():\n",
    "            selected_word = gr.Dropdown(label=\"단어를 선택하세요\", choices=word_list)\n",
    "            video_output = gr.Video(label=\"동영상\")\n",
    "            image_output = gr.Image(label=\"이미지\")\n",
    "            description_output = gr.Textbox(label=\"설명\", interactive=False)\n",
    "            \n",
    "            # 이벤트 연결\n",
    "            selected_word.change(fetch_media, inputs=[selected_word], outputs=[video_output, image_output, description_output])\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = \"이거 사용.CSV\"\n",
    "\n",
    "# CSV 파일 읽기\n",
    "data = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 채팅 및 미디어 출력 함수\n",
    "def chat_with_openai_and_fetch_media(user_input, chat_history):\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "        # 응답 추출\n",
    "        assistant_reply = completion.choices[0].message.content\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        # CSV 파일에서 단어에 해당하는 동영상 및 이미지 찾기\n",
    "        row = data[data[\"단어명\"] == user_input.strip()]\n",
    "        if not row.empty:\n",
    "            video_url = row[\"동영상\"].values[0]\n",
    "            image_url = row[\"이미지\"].values[0]\n",
    "        else:\n",
    "            video_url = None\n",
    "            image_url = None\n",
    "\n",
    "        # 대화 기록 업데이트\n",
    "        chat_history.append((user_input, assistant_reply))\n",
    "\n",
    "        # 반환: 대화 기록, 동영상 URL, 이미지 URL\n",
    "        return chat_history, video_url, image_url\n",
    "\n",
    "    except Exception as e:\n",
    "        return [(\"Error\", str(e))], None, None\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Azure OpenAI + 수어 학습 프로그램\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # 왼쪽: 채팅 및 음성 기능\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            # 대화 기록 저장\n",
    "            state = gr.State([])\n",
    "\n",
    "            # 이벤트 연결\n",
    "            user_input.submit(chat_with_openai_and_fetch_media, [user_input, state], [chatbot, state])\n",
    "            clear_button.click(lambda: [], None, chatbot)\n",
    "\n",
    "        # 오른쪽: 동영상, 이미지 출력\n",
    "        with gr.Column():\n",
    "            video_output = gr.Video(label=\"동영상\")\n",
    "            image_output = gr.Image(label=\"이미지\")\n",
    "            \n",
    "            # 채팅 입력에 따라 동영상 및 이미지 업데이트\n",
    "            user_input.submit(chat_with_openai_and_fetch_media, [user_input, state], [chatbot, video_output, image_output])\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = \"이거 사용.CSV\"\n",
    "\n",
    "# CSV 파일 읽기\n",
    "data = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 단어 리스트 추출\n",
    "word_list = data[\"단어명\"].tolist()\n",
    "\n",
    "# 단어에 따른 동영상과 이미지 불러오기 함수\n",
    "def fetch_media(selected_word):\n",
    "    # 선택된 단어에 해당하는 행 필터링\n",
    "    row = data[data[\"단어명\"] == selected_word]\n",
    "    \n",
    "    if not row.empty:\n",
    "        video_url = row[\"동영상\"].values[0]\n",
    "        image_url = row[\"이미지\"].values[0]\n",
    "        description = row[\"설명\"].values[0]\n",
    "        return video_url, image_url, description\n",
    "    else:\n",
    "        return None, None, \"해당 단어를 찾을 수 없습니다.\"\n",
    "\n",
    "# 채팅 함수 정의\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content\n",
    "        chat_history.append((user_input, assistant_reply))  # 대화 기록 저장\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, chat_history\n",
    "\n",
    "    except Exception as e:\n",
    "        return [(\"Error\", str(e))], chat_history\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Azure OpenAI + 수어 학습 프로그램\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # 왼쪽: 채팅 및 음성 기능\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            \n",
    "            # 대화 기록 저장\n",
    "            state = gr.State([])\n",
    "\n",
    "            # 이벤트 연결\n",
    "            user_input.submit(chat_with_openai, [user_input, state], [chatbot, state])\n",
    "            clear_button.click(lambda: [], None, chatbot)\n",
    "\n",
    "        # 오른쪽: 단어 선택, 동영상, 이미지 출력\n",
    "        with gr.Column():\n",
    "            video_output = gr.Video(label=\"동영상\")\n",
    "            image_output = gr.Image(label=\"이미지\")\n",
    "            \n",
    "            # 채팅 입력에 따라 동영상 및 이미지 업데이트\n",
    "            user_input.submit(chat_with_openai_and_fetch_media, [user_input, state], [chatbot, video_output, image_output])\n",
    "\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
