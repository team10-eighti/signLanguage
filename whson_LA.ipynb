{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-cognitiveservices-vision-customvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio_modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "# 채팅\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    # 수어가 있는 경우\n",
    "    try:\n",
    "        global is_speaking\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()  # 속성 접근 방식으로 수정\n",
    "\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        video_url = citations[-2]\n",
    "        image_urls = citations[-3].split(' ')  # 이미지 URL이 여러 개일 수 있음\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    # 지문자로 대체하는 경우\n",
    "    except Exception as e:\n",
    "        # 오류 메시지 대체\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "\n",
    "        return chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"Draw bounding boxes on the image based on predictions\"\"\"\n",
    "    img = image.copy()\n",
    "    for pred in predictions:\n",
    "        if pred.probability > 0.9 :\n",
    "            color = (255, 0, 0)\n",
    "            box = pred.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "        \n",
    "        # Draw rectangle\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "        \n",
    "        # Add label with confidence score\n",
    "            label = f\"{pred.tag_name}: {pred.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, pred.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        # logger.info(sign_confirmed)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "        # logger.info(sign_confirmed)\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "            # logger.info(sign_confirmed)\n",
    "\n",
    "        logger.info(sign_confirmed_str_list)\n",
    "\n",
    "        # for confirmed in sign_confirmed_str_list:\n",
    "        #     if confirmed == \"True\":\n",
    "        #         logger.info(str(sign_confirmed_str_list))\n",
    "\n",
    "        confirmed_result= []\n",
    "        for i in range(len(split_sign)):\n",
    "            confirmed_result.append(\"False\")\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        for sign in split_sign:\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "            confirmed = (sign_confirmed_str_list[i] == 'True')\n",
    "            logger.info(confirmed)            \n",
    "            if confirmed:\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result[i] = \"True\"\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"True\"\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result[i] = \"False\"\n",
    "            i = i+1\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "        with gr.Column(visible=False) as column1:   # 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "        with gr.Column(visible=False) as column2:   # 기본적으로 숨김\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            show_btn = gr.Button(\"힌트\")\n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "            demo.title = \"Azure Custom Vision Object Detection\"\n",
    "            demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "            gr.Textbox(\"이미지가 보여질 공간\")\n",
    "\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "# 채팅\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    # 수어가 있는 경우\n",
    "    try:\n",
    "        global is_speaking\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()  # 속성 접근 방식으로 수정\n",
    "\n",
    "        chat_history.append((user_input, assistant_reply))  # 튜플 형태로 추가\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        video_url = citations[-2]\n",
    "        image_urls = citations[-3].split(' ')  # 이미지 URL이 여러 개일 수 있음\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기 (텍스트를 한국어 음성으로 변환)\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)\n",
    "\n",
    "        return chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    # 지문자로 대체하는 경우\n",
    "    except Exception as e:\n",
    "        # 오류 메시지 대체\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "\n",
    "        return chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "        # logger.info(str(highest_prediction.tag_name) + \" \" + str(highest_prediction.probability))\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "            # logger.info(sign_confirmed)\n",
    "\n",
    "        logger.info(sign_confirmed_str_list)\n",
    "\n",
    "        confirmed_result= []\n",
    "\n",
    "        for i, sign in enumerate(split_sign):\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "\n",
    "            if sign_confirmed_str_list[i] == 'True':\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result.append(\"True\")\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result.append(\"True\")\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result.append(\"False\")\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "        with gr.Column(visible=False) as column1:   # 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "        with gr.Column(visible=False) as column2:   # 기본적으로 숨김\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=True)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            show_btn = gr.Button(\"힌트\")\n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "            demo.title = \"Azure Custom Vision Object Detection\"\n",
    "            demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "            gr.Textbox(\"이미지가 보여질 공간\")\n",
    "\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\blocks.py:1776: UserWarning: A function (stop_speech) returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n",
      "    Output components:\n",
      "        []\n",
      "    Output values returned:\n",
      "        [\"Speech stopped.\"]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-23 09:41:02.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mclick_sign_send\u001b[0m:\u001b[36m214\u001b[0m - \u001b[1m소맥 ['ㅅ', 'ㅗ', 'ㅁ', 'ㅐ', 'ㄱ']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:12.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:14.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:16.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:18.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:19.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:20.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'False', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:22.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:23.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:25.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:26.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:27.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:29.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:30.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:32.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:33.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:34.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:36.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:37.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:38.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:40.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'False']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:41.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:43.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:44.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:45.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:47.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:49.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:50.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'True', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:51.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:53.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'True', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:54.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:55.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'True', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:57.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:41:59.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'True', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:42:01.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:42:02.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'True', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:42:04.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:42:05.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['False', 'True', 'False', 'True', 'True']\u001b[0m\n",
      "\u001b[32m2024-12-23 09:42:06.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1m['True', 'True', 'True', 'False', 'True']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "\n",
    "###단어 자음/모음 분할 함수\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    global is_speaking\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()\n",
    "        chat_history.append((user_input, assistant_reply))\n",
    "\n",
    "        # 출처와 관련된 내용 확인\n",
    "        citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        video_url = citations[-2]\n",
    "        image_urls = citations[-3].split(' ')\n",
    "\n",
    "        # Azure Speech Service로 응답 읽기\n",
    "        is_speaking = True\n",
    "        speech_synthesizer.speak_text_async(assistant_reply)  # 여기에만 음성 합성 호출\n",
    "\n",
    "        # return1을 반환 (예외가 발생하지 않은 경우)\n",
    "        result1 = (chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False))\n",
    "        return result1  # 정상 흐름에서는 return1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        chat_history.append((user_input, fallback_message))\n",
    "        is_speaking = True\n",
    "        \n",
    "        # 예외 발생 시 fallback_message만 음성 합성\n",
    "        speech_synthesizer.speak_text_async(fallback_message)  # 음성 합성 호출\n",
    "\n",
    "        # return2를 반환 (예외 발생 시)\n",
    "        result2 = (chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True))\n",
    "        return result2  # 예외 발생 시에는 return2\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "        # logger.info(str(highest_prediction.tag_name) + \" \" + str(highest_prediction.probability))\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "            # logger.info(sign_confirmed)\n",
    "\n",
    "        logger.info(sign_confirmed_str_list)\n",
    "\n",
    "        confirmed_result= []\n",
    "\n",
    "        for i, sign in enumerate(split_sign):\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "\n",
    "            if sign_confirmed_str_list[i] == 'True':\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result.append(\"True\")\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result.append(\"True\")\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result.append(\"False\")\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "        with gr.Column(visible=False) as column1:   # 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "        with gr.Column(visible=False) as column2:   # 기본적으로 숨김\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=False)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "                show_btn = gr.Button(\"힌트\")\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            \n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "            demo.title = \"Azure Custom Vision Object Detection\"\n",
    "            demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "            gr.Textbox(\"이미지가 보여질 공간\")\n",
    "\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확인 []\n",
      "확인 [{'content': '기타\\n오른 주먹의 1지를 펴서 반쯤 구부려 끝이 오른쪽 귀로 향하게 하여 좌우로 두 번 움직인 다음, 펴서 세운 왼 주먹의 5지 등에 오른 주먹의 1·2지를 펴서 등이 위로 향하게 하여 대고 왼쪽에서 오른쪽으로 움직인다.\\nhttp://sldict.korean.go.kr/multimedia/multimedia_files/convert/20151230/237175/IMG000232857_700X466.jpg http://sldict.korean.go.kr/multimedia/multimedia_files/convert/20151230/237176/IMG000232858_700X466.jpg\\nhttp://sldict.korean.go.kr/multimedia/multimedia_files/convert/20220801/1004437/MOV000359630_700X466.mp4\\n정보', 'title': None, 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': '기타\\n모로 세운 왼 주먹의 밑면을, 오른 주먹의 1지를 펴서 윗면으로 스쳐 낸다.\\nhttp://sldict.korean.go.kr/multimedia/multimedia_files/convert/20151217/229384/IMG000222762_700X466.jpg\\nhttp://sldict.korean.go.kr/multimedia/multimedia_files/convert/20191029/632415/MOV000250158_700X466.mp4\\n의미', 'title': None, 'url': None, 'filepath': None, 'chunk_id': '0'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from gradio_modal import Modal\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Azure OpenAI 및 Cognitive Search 설정\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://team10-eighti.openai.azure.com/\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o-2\")\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\", \"https://team10-eighti-search.search.windows.net\")\n",
    "search_key = os.getenv(\"SEARCH_KEY\", \"wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3\")\n",
    "search_index = os.getenv(\"SEARCH_INDEX_NAME\", \"sign-full-index\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw\")\n",
    "\n",
    "# Azure Speech Service 설정 (한국어 음성 출력)\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\", \"AwjVcsBAkpnrMNwkobsgJ4SSroO1GkAztrEIYp1JuMTcuKcfDR3wJQQJ99ALACYeBjFXJ3w3AAAYACOGZMDc\")\n",
    "speech_region = os.getenv(\"AZURE_SPEECH_REGION\", \"eastus\")\n",
    "speech_config = SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_synthesis_language = \"ko-KR\"  # 한국어 설정\n",
    "speech_config.speech_synthesis_voice_name = \"ko-KR-SunHiNeural\"  # 한국어 음성\n",
    "speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Initialize the prediction client\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "\n",
    "# 역딕셔너리 생성 (값 -> 키 변환용)\n",
    "hangul_dict = {\"Giyeok\": \"ㄱ\", \"Nieun\": \"ㄴ\", \"Digeut\": \"ㄷ\", \"Rieul\": \"ㄹ\", \"Mieum\": \"ㅁ\", \"Bieup\": \"ㅂ\", \"Siot\": \"ㅅ\",\n",
    "        \"Ieung\": \"ㅇ\", \"Jieut\": \"ㅈ\", \"Chieut\": \"ㅊ\", \"Kieuk\": \"ㅋ\", \"Tieut\": \"ㅌ\", \"Pieup\": \"ㅍ\", \"Hieut\": \"ㅎ\",\n",
    "        \"A\": \"ㅏ\", \"Ya\": \"ㅑ\", \"Eo\": \"ㅓ\", \"Yeo\": \"ㅕ\", \"O\": \"ㅗ\", \"Yo\": \"ㅛ\", \"U\": \"ㅜ\", \"Yu\": \"ㅠ\", \"Eu\": \"ㅡ\",\n",
    "        \"Yi\": \"ㅣ\", \"Ae\": \"ㅐ\", \"Yae\": \"ㅒ\", \"E\": \"ㅔ\", \"Ye\": \"ㅖ\", \"Oe\": \"ㅚ\", \"Wi\": \"ㅟ\", \"Ui\": \"ㅢ\"}\n",
    "\n",
    "reverse_hangul_dict = {value: key for key, value in hangul_dict.items()}\n",
    "\n",
    "\n",
    "###단어 자음/모음 분할 함수\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "# Azure OpenAI 클라이언트 초기화\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# 음성 멈춤 기능을 위한 전역 변수\n",
    "is_speaking = False\n",
    "\n",
    "# 음성을 멈추는 함수\n",
    "def stop_speech():\n",
    "    global is_speaking\n",
    "    if is_speaking:\n",
    "        speech_synthesizer.stop_speaking_async()\n",
    "        is_speaking = False\n",
    "    return \"Speech stopped.\"\n",
    "\n",
    "\n",
    "def chat_with_openai(user_input, chat_history):\n",
    "    global is_speaking\n",
    "    try:\n",
    "        # 대화 기록 포함 메시지 구성\n",
    "        messages = [{\"role\": \"system\", \"content\": \"너는 수화를 알려주는 전문가야\"}]\n",
    "        for user_msg, assistant_msg in chat_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Azure OpenAI API 호출\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            max_tokens=800,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{search_endpoint}\",\n",
    "                        \"index_name\": search_index,\n",
    "                        \"semantic_configuration\": \"sign-full-semantic\",\n",
    "                        \"query_type\": \"semantic\",\n",
    "                        \"fields_mapping\": {},\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": \"너는 수화를 알려주는 전문가야\",\n",
    "                        \"filter\": None,\n",
    "                        \"strictness\": 3,\n",
    "                        \"top_n_documents\": 5,\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{search_key}\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 응답 추출 및 대화 기록 업데이트\n",
    "        assistant_reply = completion.choices[0].message.content.replace(' [doc1]', '').strip()\n",
    "        \n",
    "\n",
    "        print(\"확인\", completion.choices[0].message.context['citations'])\n",
    "        \n",
    "        citations_context = completion.choices[0].message.context['citations']\n",
    "        \n",
    "        if len(citations_context) >0:\n",
    "            citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "            video_url = citations[-2]\n",
    "            image_urls = citations[-3].split(' ')\n",
    "            chat_history.append((user_input, assistant_reply))\n",
    "\n",
    "            # Azure Speech Service로 응답 읽기\n",
    "            is_speaking = True\n",
    "            speech_synthesizer.speak_text_async(assistant_reply)  # 여기에만 음성 합성 호출\n",
    "\n",
    "            # return1을 반환 (예외가 발생하지 않은 경우)\n",
    "            result1 = (chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False))\n",
    "            return result1  # 정상 흐름에서는 return1\n",
    "        else:\n",
    "            fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "            chat_history.append((user_input, fallback_message))\n",
    "            is_speaking = True\n",
    "            \n",
    "            \n",
    "            # 예외 발생 시 fallback_message만 음성 합성\n",
    "            speech_synthesizer.speak_text_async(fallback_message)  # 음성 합성 호출\n",
    "\n",
    "            # return2를 반환 (예외 발생 시)\n",
    "            result2 = (chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True))\n",
    "            return result2  # 예외 발생 시에는 return2\n",
    "\n",
    "        # # # 출처와 관련된 내용 확인\n",
    "        # # citations = completion.choices[0].message.context['citations'][0]['content'].split('\\n')\n",
    "        # # video_url = citations[-2]\n",
    "        # # image_urls = citations[-3].split(' ')\n",
    "      \n",
    "        # # Azure Speech Service로 응답 읽기\n",
    "        # is_speaking = True\n",
    "        # speech_synthesizer.speak_text_async(assistant_reply)  # 여기에만 음성 합성 호출\n",
    "\n",
    "        # # return1을 반환 (예외가 발생하지 않은 경우)\n",
    "        # result1 = (chat_history, '', video_url, image_urls, gr.update(visible=True), gr.update(visible=False))\n",
    "        # return result1  # 정상 흐름에서는 return1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "        # fallback_message = \"수어로 확인하기 어려운 단어입니다. 지문자로 연습해보세요!\"\n",
    "        # chat_history.append((user_input, fallback_message))\n",
    "        # is_speaking = True\n",
    "        \n",
    "        # # 예외 발생 시 fallback_message만 음성 합성\n",
    "        # speech_synthesizer.speak_text_async(fallback_message)  # 음성 합성 호출\n",
    "\n",
    "        # # return2를 반환 (예외 발생 시)\n",
    "        # result2 = (chat_history, '', None, [], gr.update(visible=False), gr.update(visible=True))\n",
    "        # return result2  # 예외 발생 시에는 return2\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # if first_char_is_inprogress:\n",
    "        # logger.info(str(highest_prediction.tag_name) + \" \" + str(highest_prediction.probability))\n",
    "            # if highest_prediction.tag_name == first_char and highest_prediction.probability > 0.7:\n",
    "                # global first_char_succeed\n",
    "                # first_char_succeed = True\n",
    "                # print(\"First character detected successfully!\")\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img, highest_prediction.tag_name\n",
    "\n",
    "def click_sign_send(sign_word):\n",
    "    split_sign = split_hangul(sign_word)\n",
    "    logger.info(sign_word + \" \" + str(split_sign))\n",
    "\n",
    "    gallery_images = []\n",
    "    sign_confirmed = []\n",
    "\n",
    "    for sign in split_sign:\n",
    "        sign_file_name = reverse_hangul_dict[sign]\n",
    "        gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\")\n",
    "        sign_confirmed.append(False)\n",
    "    \n",
    "    return gr.update(visible=False), gr.update(visible=True), gallery_images, sign_confirmed\n",
    "\n",
    "# Custom Vision\n",
    "def process_frame(frame, gallery_origin_images, sign_word, sign_confirmed):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame, tag_name = draw_boxes(frame, results.predictions)\n",
    "\n",
    "        gallery_images = []\n",
    "\n",
    "        split_sign = split_hangul(sign_word)\n",
    "        sign_confirmed = sign_confirmed.replace(\"[\",\"\").replace(\"]\", \"\")\n",
    "\n",
    "        sign_confirmed_list = sign_confirmed.split(',')\n",
    "        sign_confirmed_str_list = []\n",
    "\n",
    "        for sign_confirmed in sign_confirmed_list:\n",
    "            sign_confirmed = sign_confirmed.replace(\"'\",\"\").replace('\"', \"\").strip()\n",
    "            sign_confirmed_str_list.append(sign_confirmed)\n",
    "            # logger.info(sign_confirmed)\n",
    "\n",
    "        logger.info(sign_confirmed_str_list)\n",
    "\n",
    "        confirmed_result= []\n",
    "\n",
    "        for i, sign in enumerate(split_sign):\n",
    "            sign_file_name = reverse_hangul_dict[sign]\n",
    "\n",
    "            if sign_confirmed_str_list[i] == 'True':\n",
    "                gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                confirmed_result.append(\"True\")\n",
    "            else:\n",
    "                if sign_file_name == tag_name:\n",
    "                    gallery_images.append(f\"./images/Correct/{sign_file_name}.jpg\") \n",
    "                    confirmed_result.append(\"True\")\n",
    "                else :    \n",
    "                    gallery_images.append(f\"./images/Basic/{sign_file_name}.jpg\") \n",
    "                    confirmed_result.append(\"False\")\n",
    "\n",
    "        return annotated_frame, gallery_images, str(confirmed_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        return frame, gallery_origin_images, sign_confirmed\n",
    "\n",
    "# Gradio 인터페이스 구성\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot()\n",
    "            user_input = gr.Textbox(label=\"Your Message\", placeholder=\"메시지를 입력하세요...\")\n",
    "            clear_button = gr.Button(\"Clear Chat\")\n",
    "            stop_button = gr.Button(\"Stop Speech\")\n",
    "        with gr.Column(visible=False) as column1:   # 기본적으로 숨김\n",
    "            video_display = gr.Video(label=\"수어 영상\")\n",
    "            image_display = gr.Gallery(label=\"수어 이미지\", columns=3)\n",
    "        with gr.Column(visible=False) as column2:   # 기본적으로 숨김\n",
    "            with gr.Row():\n",
    "                sign_input_textbox = gr.Textbox(label=\"지문자 입력 (2글자 단어)\", value=\"\", scale=7)\n",
    "                sign_confirmed_textbox = gr.Textbox(show_label=False, visible=False)\n",
    "                toggle_button = gr.Button(\"지문자 확인\", scale=1)\n",
    "                show_btn = gr.Button(\"힌트\")\n",
    "            with gr.Row():  # Row layout for input and output\n",
    "                gallery = gr.Gallery(columns=[6], rows=[1], show_label=False, show_share_button=False, show_download_button=False, interactive=False, show_fullscreen_button=False, height=130)\n",
    "            with gr.Row():\n",
    "                webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                webcam_output = gr.Image(label=\"Detected Objects\")\n",
    "            \n",
    "            webcam_input.stream(process_frame, inputs=[webcam_input, gallery, sign_input_textbox, sign_confirmed_textbox], outputs=[webcam_output, gallery, sign_confirmed_textbox])\n",
    "            demo.title = \"Azure Custom Vision Object Detection\"\n",
    "            demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "        \n",
    "    with Modal(visible=False) as modal:  # 지문자 힌트 보여주기\n",
    "        gr.Image(value=\"images/hint.jpg\", label=\"지문자 힌트\")  # 이미지 경로에 맞게 설정\n",
    "\n",
    "\n",
    "    # 대화 기록 저장\n",
    "    state = gr.State([])\n",
    "\n",
    "    # 이벤트 연결\n",
    "    user_input.submit(chat_with_openai, [user_input, state], [chatbot, user_input, video_display, image_display, column1, column2])\n",
    "    clear_button.click(\n",
    "        lambda: ([], []),  # chatbot과 state를 모두 초기화\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state]  # chatbot과 state 모두 업데이트\n",
    "    )\n",
    "    stop_button.click(stop_speech)  # 음성을 멈추기\n",
    "    show_btn.click(lambda: Modal(visible=True), None, modal)  # 지문자 힌트 보여주기\n",
    "    toggle_button.click(fn=click_sign_send, inputs=[sign_input_textbox], outputs=[column1, column2, gallery, sign_confirmed_textbox])\n",
    "\n",
    "# 실행\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
