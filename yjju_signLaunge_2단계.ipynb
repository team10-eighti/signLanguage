{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\components\\base.py:201: UserWarning: 'scale' value should be an integer. Using 0.3 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7917\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7917/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소맥\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-19 17:02:19.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrequest_gpt\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mcontent_text = 요청된 정보는 검색된 데이터에서 찾을 수 없습니다. 다른 쿼리나 주제를 시도해 보세요.\u001b[0m\n",
      "\u001b[32m2024-12-19 17:02:19.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrequest_gpt\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mcitations(empty) = []\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9696\\203385096.py\", line 244, in click_send\n",
      "    response_text, citations_movie, citations_images_first, citations_images_second = request_gpt(prompt)\n",
      "                                                                                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9696\\203385096.py\", line 204, in request_gpt\n",
      "    toggle_visibility()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9696\\203385096.py\", line 296, in toggle_visibility\n",
      "    demo.set_event(toggle_columns())\n",
      "    ^^^^^^^^^^^^^^\n",
      "AttributeError: 'Blocks' object has no attribute 'set_event'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gradio as gr\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "from loguru import logger\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "\n",
    "ENDPOINT = \"https://team10eighticustomvision-prediction.cognitiveservices.azure.com/\"\n",
    "PREDICTION_KEY = \"9FRZbiwBubFIcSZ1k88tCTCskOAZwMMAMvnFLuVJ26tlU0V0fsqjJQQJ99ALACYeBjFXJ3w3AAAIACOGOj1S\"\n",
    "PROJECT_ID = \"4218ecac-688a-422b-9e14-2726b938f67c\"\n",
    "PUBLISHED_NAME = \"Iteration8\"\n",
    " \n",
    "# Initialize the prediction client\n",
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(endpoint=ENDPOINT, credentials=credentials)\n",
    "# 상태 변수\n",
    "col1_state = True\n",
    "\n",
    "def draw_boxes(image, predictions):\n",
    "    \"\"\"가장 확률이 높은 객체만 경계 상자를 그리도록 수정\"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # 예측 결과 중 확률이 가장 높은 하나를 선택\n",
    "    if predictions:\n",
    "        highest_prediction = max(predictions, key=lambda p: p.probability)\n",
    "        \n",
    "        # 확률이 0.5 이상인 객체만 선택\n",
    "        if highest_prediction.probability > 0.5:\n",
    "            color = (255, 0, 0)  # 경계 상자 색상 (빨간색)\n",
    "            box = highest_prediction.bounding_box\n",
    "            left = int(box.left * img.shape[1])\n",
    "            top = int(box.top * img.shape[0])\n",
    "            width = int(box.width * img.shape[1])\n",
    "            height = int(box.height * img.shape[0])\n",
    "\n",
    "            # 경계 상자 그리기\n",
    "            cv2.rectangle(img, \n",
    "                        (left, top), \n",
    "                        (left + width, top + height), \n",
    "                        color, \n",
    "                        2)\n",
    "            \n",
    "            # 라벨과 확률 텍스트 추가\n",
    "            label = f\"{highest_prediction.tag_name}: {highest_prediction.probability:.2f}\"\n",
    "            cv2.putText(img, \n",
    "                        label, \n",
    "                        (left, top - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, \n",
    "                        color, \n",
    "                        2)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def process_frame(frame):\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert frame to PIL Image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    \n",
    "    # Save to bytes for Azure API\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "\n",
    "    try:\n",
    "        # Get predictions from Azure Custom Vision\n",
    "        results = predictor.detect_image(PROJECT_ID, PUBLISHED_NAME, img_byte_arr)\n",
    "        \n",
    "        # Draw boxes on frame\n",
    "        annotated_frame = draw_boxes(frame, results.predictions)\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return frame\n",
    "    \n",
    "def toggle_visibility(col1_visible, col2_visible):\n",
    "    # 교차로 visible 상태 업데이트\n",
    "    col1_new_visibility = not col1_visible\n",
    "    col2_new_visibility = not col2_visible\n",
    "    return (col1_new_visibility, col2_new_visibility,\n",
    "            gr.update(visible=col1_new_visibility),\n",
    "            gr.update(visible=col2_new_visibility))\n",
    "\n",
    "def split_hangul(word):\n",
    "    # 한글 초성, 중성, 종성 리스트\n",
    "    initial_consonants = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    medial_vowels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "    final_consonants = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    " \n",
    "    separated = []\n",
    "    for char in word:\n",
    "        if '가' <= char <= '힣':  # 한글 음절인지 확인\n",
    "            code = ord(char) - ord('가')\n",
    "            initial = code // (21 * 28)  # 초성\n",
    "            medial = (code % (21 * 28)) // 28  # 중성\n",
    "            final = code % 28  # 종성\n",
    "            separated.append(initial_consonants[initial])  # 초성 추가\n",
    "            separated.append(medial_vowels[medial])       # 중성 추가\n",
    "            if final_consonants[final]:                   # 종성이 있다면 추가\n",
    "                separated.append(final_consonants[final])\n",
    "        else:\n",
    "            separated.append(char)  # 한글이 아닌 문자는 그대로 추가\n",
    " \n",
    "    return separated\n",
    "\n",
    "def request_gpt(user_text, temperature=0.7, top_p=0.95, max_tokens=800):\n",
    "\n",
    "    # endpoint\n",
    "    # method\n",
    "    # header\n",
    "    # payload\n",
    "\n",
    "    api_base=\"https://team10-eighti.openai.azure.com\" \n",
    "    deployment_id=\"gpt-4o\"  \n",
    "    endpoint = f\"{api_base}/openai/deployments/{deployment_id}/chat/completions?api-version=2024-08-01-preview\"\n",
    "    api_key = '1BajXTI5Mp0tKrs46XFGuOWjSPXKzOZSKy8e6R3qha1SVQ4lz1PFJQQJ99ALACYeBjFXJ3w3AAABACOGgPvw'\n",
    "    \n",
    "    search_endpoint=\"https://team10-eighti-search.search.windows.net\"\n",
    "    search_key = 'wnalAsW6FqKRHIR6S3sUZGzNH28Lf3sBOS2ubCZsZxAzSeA205k3'\n",
    "    search_index=\"sign-index\"\n",
    "    semantic_name = \"sign-semantic\"\n",
    "\n",
    "    method = requests.post\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application.json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 사용자가 수화 정보를 찾는데 도움을 주는 수화 AI 도우미입니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_text\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stop\": None,\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": search_endpoint,\n",
    "                    \"index_name\": search_index,\n",
    "                    \"semantic_configuration\": semantic_name,\n",
    "                    \"query_type\": \"semantic\",\n",
    "                    \"fields_mapping\": {},\n",
    "                    \"filter\": None,\n",
    "                    \"top_n_documents\": 5,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": search_key\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=payload)\n",
    "    # print(response.status_code)\n",
    "    # print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        content_text = response_json['choices'][0]['message']['content']\n",
    "        logger.info(f\"content_text = {content_text}\")\n",
    "\n",
    "        # citations = response_json['choices'][0]['message']['context']['citations'][0]['content'].split('\\n')\n",
    "        citations = response_json['choices'][0]['message']['context']['citations']\n",
    "        if len(citations) > 0:\n",
    "            logger.info(f\"citations = {citations}\")\n",
    "            citations_movie = get_citations(citations, \"mp4\")\n",
    "            citations_images = get_citations(citations, \"jpg\").split(' ')\n",
    "            logger.info(f\"citations_movie = {citations_movie}\")\n",
    "            logger.info(f\"citations_images = {citations_images}\")\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "            if (len(citations_images) > 0 ):\n",
    "                citations_images_first = get_image_url(citations_images[0])\n",
    "\n",
    "                print(citations_images_first)\n",
    "                if (len(citations_images) > 1 ):\n",
    "                    citations_images_second = citations_images[1]\n",
    "        else:\n",
    "            logger.info(f\"citations(empty) = {citations}\")\n",
    "            \n",
    "            toggle_visibility()\n",
    "\n",
    "            citations_movie = \"No Video\"\n",
    "            citations_images_first = None\n",
    "            citations_images_second = None\n",
    "\n",
    "        return content_text, citations_movie, citations_images_first, citations_images_second\n",
    "    else:\n",
    "        return \"\", \"No Video\", None, None\n",
    "\n",
    "def get_citations(citations=\"\", extentions=\"mp4\"):\n",
    "    logger.info(f\"****get_citations = {citations}\")\n",
    "    if len(citations) > 0 and len(citations[0]) > 0 and len(citations[0]['content']) > 0:\n",
    "        contents = citations[0]['content'].split('\\n')\n",
    "\n",
    "        for row in contents:\n",
    "            row_list = row.split('.')\n",
    "            if row_list[-1] == extentions:\n",
    "                print(\"extentions=\", extentions, \"|row=\",  row)\n",
    "                return row\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def get_image_url(url):\n",
    "\n",
    "    if not url.strip():\n",
    "        return None  # 빈값 처리\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # HTTP 에러 확인\n",
    "        img = Image.open(BytesIO(response.content))  # 이미지를 메모리에서 읽음\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching image: {str(e)}\"\n",
    "\n",
    "def click_send(prompt, histories):\n",
    "    print(prompt)\n",
    "    print(histories)\n",
    "    # history_list = get_history_messages(histories=histories)\n",
    "    # response_text, citation_html = request_gpt(prompt, history_list)\n",
    "    response_text, citations_movie, citations_images_first, citations_images_second = request_gpt(prompt)\n",
    "    histories.append((prompt, response_text))\n",
    "    return histories, \"\", citations_movie, citations_images_first, citations_images_second\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # 두 개의 상태\n",
    "    col1_state = gr.State(value=True)  # Column 1 초기 visible 상태\n",
    "    col2_state = gr.State(value=False)  # Column 2 초기 visible 상태\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(label='채팅 기록')\n",
    "            with gr.Row():\n",
    "                input_textbox = gr.Textbox(label=\"\", scale=7)\n",
    "                send_button = gr.Button(\"전송\", scale=1)\n",
    "\n",
    "        # citation = gr.HTML(label='참조')\n",
    "        with gr.Column():\n",
    "            toggle_button = gr.Button(\"Toggle Columns\")\n",
    "            with gr.Column(visible=True) as column1:\n",
    "                videio = gr.Video(label=\"Video Player\", autoplay=True)\n",
    "                with gr.Row():\n",
    "                    image_first = gr.Image(scale=0.3)\n",
    "                    image_second = gr.Image(scale=0.3)\n",
    "\n",
    "            with gr.Column(visible=False) as column2:\n",
    "                with gr.Row():  # Row layout for input and output\n",
    "                    with gr.Column():  # Input webcam column\n",
    "                        webcam_input = gr.Image(sources=\"webcam\", streaming=True, mirror_webcam=True, label=\"Webcam\")\n",
    "                    with gr.Column():  # Output display column\n",
    "                        output = gr.Image(label=\"Detected Objects\")\n",
    "\n",
    "                # Set the process_frame function as the update function for webcam input\n",
    "                webcam_input.stream(process_frame, inputs=webcam_input, outputs=output)\n",
    "                \n",
    "                demo.title = \"Azure Custom Vision Object Detection\"\n",
    "                demo.description = \"Real-time object detection using Azure Custom Vision\"\n",
    "    # 상태 변경 함수\n",
    "    def toggle_columns():\n",
    "        global col1_state\n",
    "        col1_state = not col1_state\n",
    "        return gr.update(visible=col1_state), gr.update(visible=not col1_state)\n",
    "\n",
    "    input_textbox.submit(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second])\n",
    "    send_button.click(fn=click_send, inputs=[input_textbox, chatbot], outputs=[chatbot, input_textbox, videio, image_first, image_second])\n",
    "    toggle_button.click(toggle_visibility, inputs=[col1_state, col2_state], outputs=[col1_state, col2_state, column1, column2])\n",
    "# content_text, citations_list = request_gpt(\"된장찌개가 수화로 어떻게 돼?\", temperature=0.7, top_p=0.95, max_tokens=800)\n",
    "\n",
    "# print(content_text)\n",
    "# print(citations_list)\n",
    "\n",
    "def toggle_visibility():\n",
    "    demo.set_event(toggle_columns())\n",
    "\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
